# Dockerfile.backend
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies (needed for Polars/TensorFlow)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code and artifacts
# We need src/ for code and processed_data/ for the model weights
COPY src/ src/
COPY processed_data/ processed_data/
# Note: We do NOT copy ml-25m/ because it's too big. 
# The inference script should rely on processed_data/ only.

# Expose the port
EXPOSE 8000

# Run the inference server
# We use python -m to run it as a module
CMD ["python", "src/04_inference.py"]